package consumer

import java.util.Properties

import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.ConnectionFactory

import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions._
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, LocationStrategies}
//import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}

object twitterConsumer {

  // HBase
  val conf = new HBaseConfiguration()
  val connection = ConnectionFactory.createConnection(conf)
  val admin = connection.getAdmin()

  val topicName = "twitterCapstone"
  val groupName = "kafkaConsumerGroup"

  val kafkaParams = Map[String, Object](
//    "bootstrap.servers" -> "localhost:9092,localhost:9093",
    "bootstrap.servers" -> "localhost:6667", // hortonworks
    "key.deserializer" -> classOf[StringDeserializer],
    "value.deserializer" -> classOf[StringDeserializer],
    "group.id" -> groupName,
//    "enable.auto.commit" -> (false: java.lang.Boolean) // Important: need to manually commit then
    "auto.offset.reset" -> "earliest"
  )

  val spark = SparkSession.builder.master("local[*]").appName("twitterCapstone")
    .getOrCreate()

  import spark.implicits._

  import spark.sqlContext.implicits._

  val sparkConf = spark.sparkContext
  // all things stream
  val ssc = new StreamingContext(sparkConf, Seconds(1))
  ssc.checkpoint("checkpoint")
  val sc = ssc.sparkContext
  sc.setLogLevel("WARN")

  // needs to be in a collection format
  val topics = topicName.split(",").map(_.trim)

  // empty dataframe and schema and stuff.
  case class twitterRow(created_at: String, text: String, screen_name: String, follower_count: Integer, friends_count: Integer, location: String)
  val schema = StructType(List(
    StructField("created_at",StringType,true),
    StructField("text",StringType,true),
    StructField("user",StructType(List(
      StructField("screen_name",StringType,true)
    )),true),
    StructField("follower_count",LongType,true),
    StructField("friends_count",LongType,true),
    StructField("location",StringType,true)
  ))
  val outSchema = StructType(List(
    StructField("created_at",StringType,true),
    StructField("text",StringType,true),
    StructField("screen_name",StringType,true),
    StructField("follower_count",LongType,true),
    StructField("friends_count",LongType,true),
    StructField("location",StringType,true)
  ))
  var df = spark.createDataFrame(spark.sparkContext.emptyRDD[Row],outSchema)

  // function
  def quitConsumer(): Unit = {
    df.collect()
    df.coalesce(1).write.mode("overwrite").json("/home/shinray/capston1json")
    println("SAVEing ❤")
  }

  def main (args: Array[String]) : Unit = {
    var counter: Int = 0
    val stream = KafkaUtils.createDirectStream(
      ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe[String,String](topics,kafkaParams))

      stream.foreachRDD(rdd => {
        if (counter % 5 == 0) quitConsumer()
        else if (counter > 120) {
          println("limit reached, stopping")
          quitConsumer()
          ssc.stop(true,true)
          println("goodbye ❤")
          System.exit(0)
        }
        counter += 1
        println(counter)

        rdd.foreach(record => {
          println(record.value())
          val tmp = spark.sparkContext.makeRDD(record.value() :: Nil)
          var testdf = spark.read.schema(schema).json(tmp)
          testdf = testdf
              .withColumn("screen_name", testdf.col("user.screen_name"))
              .drop("user")
          df = df.unionByName(testdf)
          df.show()
        })
      })

    ssc.start()
    ssc.awaitTermination()
  }
}
